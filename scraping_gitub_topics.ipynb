{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8GLMLKjiRceuiaePKFHiO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poojasharma021/scraping-github-topics-repositories/blob/main/scraping_gitub_topics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping Top Repostiories for Topics on Github"
      ],
      "metadata": {
        "id": "89fmOnOMsNLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the steps we'll follow:\n",
        "\n",
        "We're going to scrape https://github.com/topics\n",
        "We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
        "For each topic, we'll get the top 25 repositories in the topic from the topic page\n",
        "For each repository, we'll grab the repo name, username, stars and repo URL\n",
        "For each topic we'll create a CSV file."
      ],
      "metadata": {
        "id": "Wk0iFcHEtrk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scrape the list of topics from Github "
      ],
      "metadata": {
        "id": "aOvHJESItywX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps:\n",
        "\n",
        "\n",
        "*   use requests to downlaod the page\n",
        "*   user BS4 to parse and extract information\n",
        "*   convert to a Pandas dataframe\n",
        "\n",
        "Let's write a function to download the page."
      ],
      "metadata": {
        "id": "B9kmaSGauC3s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jhov36g0Ygl4"
      },
      "outputs": [],
      "source": [
        "!pip install requests --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 --quiet"
      ],
      "metadata": {
        "id": "Q7-_bvOAcjN3"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "4jDTbNNAc56x"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_titles(parsed_doc):\n",
        "    title_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
        "    topic_title_tags = parsed_doc.find_all('p', {'class' : title_class})\n",
        "    topic_titles = []\n",
        "    for tag in topic_title_tags:\n",
        "      topic_titles.append(tag.text)\n",
        "    return topic_titles\n"
      ],
      "metadata": {
        "id": "cVt5sA6OcTII"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_desc(parsed_doc):\n",
        "  desc_class = 'f5 color-fg-muted mb-0 mt-1'\n",
        "  topic_desc_tags = parsed_doc.find_all('p', {'class': desc_class})\n",
        "  topic_descriptions = []\n",
        "  for tag in topic_desc_tags:\n",
        "    topic_descriptions.append(tag.text.strip())\n",
        "  return topic_descriptions\n",
        "\n"
      ],
      "metadata": {
        "id": "kKxHoqhFcrQH"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_url(parsed_doc):\n",
        "  link_class = 'no-underline flex-1 d-flex flex-column'\n",
        "  topic_link_tags = parsed_doc.find_all('a', {'class': link_class})\n",
        "  topic_url = []\n",
        "  for tag in topic_link_tags:\n",
        "    topic_url.append(\"https://github.com\" + tag['href'])\n",
        "  return topic_url\n",
        "\n"
      ],
      "metadata": {
        "id": "Xysj0EOAc8nD"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_topics():\n",
        "  topics_url = 'https://github.com/topics'\n",
        "  response = requests.get(topics_url)\n",
        "\n",
        "  if response.status_code != 200:\n",
        "    raise Exception('Failed to load page {}'.format(topic_url))\n",
        "  \n",
        "  parsed_doc = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "  topics_dict = {\n",
        "    'Title': get_topic_titles(parsed_doc),\n",
        "    'Descriptions': get_topic_desc(parsed_doc),\n",
        "    'Url': get_url(parsed_doc)\n",
        "   }\n",
        "\n",
        "\n",
        "  return pd.DataFrame(topics_dict)\n"
      ],
      "metadata": {
        "id": "T0pp80lsNdyW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the top repositories from a topic page"
      ],
      "metadata": {
        "id": "r2SX7hHupC6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_star_count(stars_str):\n",
        "  stars_str = stars_str.strip()\n",
        "  if stars_str[-1] == 'k':\n",
        "    return int(float(stars_str[:-1])*1000)\n",
        "  return int(stars_str)\n"
      ],
      "metadata": {
        "id": "6H98YdpAtSO1"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_repo_info(h3_tag, star_tag):\n",
        "  a_tags = h3_tag.find_all('a')\n",
        "  username = a_tags[0].text.strip()\n",
        "  repo_name = a_tags[1].text.strip()\n",
        "  repo_url = \"https://github.com\"+ a_tags[1]['href']\n",
        "  stars = parse_star_count(star_tag.text.strip())\n",
        "  return username, repo_name,stars, repo_url\n"
      ],
      "metadata": {
        "id": "5MttgNyipCoU"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_repos(topic_url):\n",
        "    # Download the page\n",
        "  response = requests.get(topic_url)\n",
        "    # Check Sucessful response\n",
        "  if response.status_code != 200:\n",
        "    raise Exception('Failed to load page {}'.format(topic_url))\n",
        "\n",
        "  topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "  repo_class ='f3 color-fg-muted text-normal lh-condensed'\n",
        "  repo_tags = topic_doc.find_all('h3', {'class': repo_class})\n",
        "\n",
        "  star_tags = topic_doc.find_all('span', {'id': 'repo-stars-counter-star'})\n",
        "\n",
        "  topic_repo_dict = {\n",
        "    'username': [], \n",
        "    'repo_name': [],\n",
        "    'stars': [],\n",
        "    'repo_url': []\n",
        "    }\n",
        "\n",
        "\n",
        "  for i in range(len(repo_tags)):\n",
        "      repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
        "      topic_repo_dict['username'].append(repo_info[0])\n",
        "      topic_repo_dict['repo_name'].append(repo_info[1])\n",
        "      topic_repo_dict['stars'].append(repo_info[2])\n",
        "      topic_repo_dict['repo_url'].append(repo_info[3])\n",
        "\n",
        "  return pd.DataFrame(topic_repo_dict)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "Pk-BRnbND4xz"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "def scrape_topic(topic_name, topic_url):\n",
        "  fname = topic_name+ '.csv'\n",
        "  if os.path.exists(fname):\n",
        "    print(\"File {} already exists.\".format(fname))\n",
        "    return\n",
        "  topic_df = get_topic_repos(topic_url)\n",
        "  topic_df.to_csv(fname, index=None)\n",
        "\n"
      ],
      "metadata": {
        "id": "kH7GMTWTnPYK"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_topic_repos():\n",
        "  print('Scraping List of Topics')\n",
        "  topics_df = scrape_topics()\n",
        "\n",
        "  for index, row in topics_df.iterrows():\n",
        "    print('Scraping Top Repositories for {}'.format(row['Title']))\n",
        "    scrape_topic(row['Title'], row['Url'])\n"
      ],
      "metadata": {
        "id": "t-bfRlExeegu"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run it to scrape the top repos for the all the topics on the first page of https://github.com/topics\n",
        "\n"
      ],
      "metadata": {
        "id": "LN0VpvpRv9UO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scrape_topic_repos()"
      ],
      "metadata": {
        "id": "suQVCoJNqo8k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}